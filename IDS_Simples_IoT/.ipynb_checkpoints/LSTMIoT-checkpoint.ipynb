{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b26b408-c7df-483d-bd14-054374c1e272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 13:31:53.441234: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#chamada \n",
    "#if __name__ == '__main__':\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools    \n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "#vamos importar as classes para realizar o random RandomOverSampler e SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras import metrics\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe4340f-a913-4386-bac3-439abcffdfff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CICIoT2023'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m DATASET_DIRECTORY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCICIoT2023\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#dataset k.endswith('.csv') -> pega somente os arquivos que terminam com .csv\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data_set \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(DATASET_DIRECTORY) \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      4\u001b[0m data_set_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(data_set)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CICIoT2023'"
     ]
    }
   ],
   "source": [
    "DATASET_DIRECTORY = 'CICIoT2023'\n",
    "#dataset k.endswith('.csv') -> pega somente os arquivos que terminam com .csv\n",
    "data_set = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "data_set_n = sorted(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212e92b-761f-45dd-bc10-19417e08ddbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54815846-0d9c-4a83-841c-7a3583948747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    DATASET_DIRECTORY = 'CICIoT2023'\n",
    "    #dataset k.endswith('.csv') -> pega somente os arquivos que terminam com .csv\n",
    "    data_set = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
    "    data_set_n = sorted(data_set)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    #pegando os dados de train\n",
    "    training_sets = data_set_n[:int(len(data_set_n)*0.8)]\n",
    "    #pegando os dados de test\n",
    "    test_sets = data_set_n[int(len(data_set_n)*0.8):]\n",
    "\n",
    "\n",
    "    \n",
    "    #dados de test\n",
    "    for test in test_sets:\n",
    "        path = DATASET_DIRECTORY+'/'+test\n",
    "        test = pd.read_csv(path)\n",
    "        new_y_test = [dict_7classes[k] for k in test['label']]\n",
    "        y_test = new_y_test\n",
    "        X_test = test.drop('label', axis=1)\n",
    "        \n",
    "\n",
    "    for train in training_sets:\n",
    "        path = DATASET_DIRECTORY+'/'+train\n",
    "        train = pd.read_csv(path)\n",
    "        new_y_train = [dict_7classes[k] for k in train['label']]\n",
    "        y_train = new_y_train\n",
    "        X_train = train.drop('label', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #plotagem dos dados normais\n",
    "    plotagem_dados(y_train)\n",
    "    #igualando os dados com técnica simples\n",
    "    X_train_sampling, y_train_sampling = RandomOver_class(X_train, y_train)\n",
    "    plotagem_dados(y_train_sampling)\n",
    "\n",
    "    #utilizando SMOTE\n",
    "    X_train_smote, y_train_smote = smote_class(X_train, y_train)\n",
    "    plotagem_dados(y_train_smote)\n",
    "\n",
    "    \n",
    "    #transformar dados de treino\n",
    "    X_train_normalizador = MinMaxScaler()\n",
    "    X_train = X_train_normalizador.fit_transform(X_train)\n",
    " \n",
    "    labelencoder_train  = LabelEncoder()\n",
    "    y_train = labelencoder_train.fit_transform(y_train)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "        \n",
    "    #transformar dados de treino sampling\n",
    "    X_train_sampling_normalizador = MinMaxScaler()\n",
    "    X_train_sampling = X_train_sampling_normalizador.fit_transform(X_train_sampling)\n",
    "    \n",
    "\n",
    "    labelencoder_train  = LabelEncoder()\n",
    "    y_train_sampling = labelencoder_train.fit_transform(y_train_sampling)\n",
    "    y_train_sampling = np_utils.to_categorical(y_train_sampling)\n",
    "\n",
    "        \n",
    "    #transformar dados de treino smote\n",
    "    X_train_smote_normalizador = MinMaxScaler()\n",
    "    X_train_smote = X_train_smote_normalizador.fit_transform(X_train_smote)\n",
    "    \n",
    "\n",
    "    labelencoder_train  = LabelEncoder()\n",
    "    y_train_smote = labelencoder_train.fit_transform(y_train_smote)\n",
    "    y_train_smote = np_utils.to_categorical(y_train_smote)\n",
    "    \n",
    "\n",
    "    \n",
    "    #transformar dados de teste entre 0 e 1\n",
    "    X_test_normalizador = MinMaxScaler()\n",
    "    X_test = X_test_normalizador.fit_transform(X_test)\n",
    "    \n",
    "    #transformar em números inteiros\n",
    "    labelencoder_teste = LabelEncoder()\n",
    "    y_test = labelencoder_teste.fit_transform(y_test)\n",
    "    #transformação em  vetores binários\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    return X_train, y_train, X_train_sampling, y_train_sampling, X_train_smote, y_train_smote, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecd2e4-a3f0-44a9-96f1-5c9eb5f254f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train_dump, X_train_sampling, y_train_sampling, X_train_smote, y_train_smote, X_test, y_test_dump = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa419b-41c5-44eb-be79-a263587f9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformando os dados 2D em 3D\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Dados 2D de exemplo\n",
    "data_2d = np.random.rand(100, 5)  # 100 amostras, 5 características\n",
    "targets = np.random.rand(100, 1)  # 100 amostras, 1 target\n",
    "\n",
    "# Usando TimeseriesGenerator para transformar dados em 3D\n",
    "timesteps = 1\n",
    "generator = TimeseriesGenerator(data_2d, targets, length=timesteps, batch_size=1)\n",
    "data_3d = np.array([x[0] for x in generator])\n",
    "\n",
    "print(\"Shape of 2D data:\", data_2d.shape)\n",
    "print(\"Shape of 3D data:\", data_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18df05-bffa-4ffb-aea9-2a13728d7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformando os dados 2D em 3D\n",
    "data_2d = np.random.rand(100, 5)  # 100 amostras, 5 características\n",
    "targets = np.random.rand(100, 1)  # 100 amostras, 1 target\n",
    "\n",
    "# Adicionando uma dimensão para timesteps\n",
    "timesteps = 1\n",
    "data_3d = np.expand_dims(data_2d, axis=1)  # Resulta em (100, 1, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95edb46-d405-4f2f-a801-dd63f7035c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749f9c0-90cc-4c3f-9674-b4ce41e5cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeloDense(X_train, metricas):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(500, activation=\"relu\", input_shape=(X_train.shape[-1],)),\n",
    "        tf.keras.layers.Dense(units=500, activation='relu', kernel_initializer=\"random_uniform\", bias_initializer=\"random_uniform\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=500, activation='relu', kernel_initializer=\"random_uniform\", bias_initializer=\"random_uniform\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(8,activation='softmax')\n",
    "        \n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(1e-3), metrics=metricas)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e5144-2343-49ad-bc89-83e4c3b2fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo\n",
    "X_train = np.random.rand(1000, 10, 5)  # 1000 amostras, 10 timesteps, 5 features\n",
    "y_train = np.random.rand(1000, 1)      # 1000 amostras, 1 output\n",
    "\n",
    "\n",
    "# Treinando o modelo\n",
    "modeloLstm = modeloLSTM()\n",
    "history = modeloLstm.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Fazendo previsões\n",
    "X_test = np.random.rand(20, 10, 5)  # 20 amostras, 10 timesteps, 5 features\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Predições:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
